<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Cache Eviction: 30 Years of Improvements | Ryan J Hamby</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
Cache hierarchies are everywhere—from L1/L2/L3 on your CPU to Redis clusters backing your microservices. But the way we organize and manage these tiers has fundamentally transformed over the past three decades. What started as simple in-memory buffers has evolved into a sophisticated science of eviction policies, write-through strategies, and predictive prefetching.
This post explores why caches are structured the way they are, how eviction strategies have improved, and what the actual data tells us about their effectiveness over time.">
<meta name="author" content="Ryan J Hamby">
<link rel="canonical" href="https://ryanjhamby.github.io/blog/cache-eviction-improvements/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ryanjhamby.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ryanjhamby.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ryanjhamby.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ryanjhamby.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ryanjhamby.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ryanjhamby.github.io/blog/cache-eviction-improvements/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><link rel="stylesheet" href="/css/custom.css">
<meta property="og:url" content="https://ryanjhamby.github.io/blog/cache-eviction-improvements/">
  <meta property="og:site_name" content="Ryan J Hamby">
  <meta property="og:title" content="Cache Eviction: 30 Years of Improvements">
  <meta property="og:description" content="Introduction Cache hierarchies are everywhere—from L1/L2/L3 on your CPU to Redis clusters backing your microservices. But the way we organize and manage these tiers has fundamentally transformed over the past three decades. What started as simple in-memory buffers has evolved into a sophisticated science of eviction policies, write-through strategies, and predictive prefetching.
This post explores why caches are structured the way they are, how eviction strategies have improved, and what the actual data tells us about their effectiveness over time.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-01-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-11T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cache Eviction: 30 Years of Improvements">
<meta name="twitter:description" content="Introduction
Cache hierarchies are everywhere—from L1/L2/L3 on your CPU to Redis clusters backing your microservices. But the way we organize and manage these tiers has fundamentally transformed over the past three decades. What started as simple in-memory buffers has evolved into a sophisticated science of eviction policies, write-through strategies, and predictive prefetching.
This post explores why caches are structured the way they are, how eviction strategies have improved, and what the actual data tells us about their effectiveness over time.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blog",
      "item": "https://ryanjhamby.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Cache Eviction: 30 Years of Improvements",
      "item": "https://ryanjhamby.github.io/blog/cache-eviction-improvements/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Cache Eviction: 30 Years of Improvements",
  "name": "Cache Eviction: 30 Years of Improvements",
  "description": "Introduction Cache hierarchies are everywhere—from L1/L2/L3 on your CPU to Redis clusters backing your microservices. But the way we organize and manage these tiers has fundamentally transformed over the past three decades. What started as simple in-memory buffers has evolved into a sophisticated science of eviction policies, write-through strategies, and predictive prefetching.\nThis post explores why caches are structured the way they are, how eviction strategies have improved, and what the actual data tells us about their effectiveness over time.\n",
  "keywords": [
    
  ],
  "articleBody": "Introduction Cache hierarchies are everywhere—from L1/L2/L3 on your CPU to Redis clusters backing your microservices. But the way we organize and manage these tiers has fundamentally transformed over the past three decades. What started as simple in-memory buffers has evolved into a sophisticated science of eviction policies, write-through strategies, and predictive prefetching.\nThis post explores why caches are structured the way they are, how eviction strategies have improved, and what the actual data tells us about their effectiveness over time.\nThe Cache Problem At its core, caching solves a fundamental asymmetry: access speeds vary wildly. Reading from memory takes ~100 nanoseconds. Reading from disk takes ~10 milliseconds. That’s a 100,000x difference. A cache’s job is to minimize trips to slower storage by keeping hot data nearby.\nBut caches have finite capacity. When full, you must decide what to evict. Get this wrong, and your hit rate craters. Get it right, and you can hide most of the latency penalty.\nHistorical Context: Why Hierarchies Exist Early computers (1980s-early 1990s) had flat memory architectures. A single level of cache—or none at all. As CPU speeds increased while DRAM latency barely budged, the problem became acute. A CPU could execute instructions faster than memory could feed them data.\nIntel’s solution, pioneered with the i486 (1989), was multi-tier caching:\nL1 Cache: Tiny (8-32KB), extremely fast (2-4 cycles), physically close to execution units L2 Cache: Larger (256KB-1MB), slower (10-20 cycles), shared or per-core L3 Cache: Larger still (4-16MB), even slower (40-75 cycles), shared across cores The insight was hierarchical: if you can’t fit everything in L1, overflow to L2. If you can’t fit in L2, overflow to L3. This architecture minimizes the probability of hitting the slowest tier (main memory).\nBy the late 1990s, this pattern was so effective that it became the de facto standard for all levels of caching—from CPU caches to database buffers to distributed caches like memcached.\nEviction Policies: The Core Problem With multiple tiers, the critical question is: which data stays, which gets evicted?\nLeast Recently Used (LRU) LRU was the dominant policy for decades. The intuition is simple: if you haven’t used something recently, you won’t need it soon. Remove the least recently accessed item when capacity is exceeded.\nAccess pattern: A, B, C, A, B, D (cache size = 3) Step 1: [A] Step 2: [A, B] Step 3: [A, B, C] Step 4: [B, C, A] (A accessed, moves to end) Step 5: [C, A, B] (B accessed, moves to end) Step 6: [A, B, D] (C evicted, D added) LRU is simple to implement and works reasonably well for temporal locality (programs access recently-used data again soon). However, it has a critical weakness: sequential scans. If you’re scanning a large array, you evict the entire cache’s contents without using them twice.\nLeast Frequently Used (LFU) LFU tracks how many times each item was accessed, evicting the least-accessed. It captures true “popularity” better than LRU.\nThe trade-off? It’s expensive. Maintaining frequency counters requires additional bookkeeping. More importantly, it’s inflexible to access pattern changes. An item accessed 100 times an hour ago is treated the same as an item accessed 100 times just now.\nVariants and Hybrids By the 2000s, the research community had developed sophisticated variants:\nClock: LRU approximation using a circular buffer and a single “hand” pointer. Much faster than true LRU. W-TinyLFU: Hybrid combining recency (LRU) and frequency (LFU). Used in Caffeine cache (Java). ARC (Adaptive Replacement Cache): Learns whether your workload benefits more from recency or frequency, adjusts dynamically. Modern caches like Redis (v4.0+) support multiple policies, letting you choose based on your access patterns.\nCache Tier Interactions: A Sequence Diagram How do these tiers actually work together? Here’s a typical read miss cascading through the hierarchy:\nCPU Core L1 Cache L2 Cache L3 Cache Main Memory | | | | | | Read 0x1234 (miss) | | | | |------- req -------------\u003e| | | | | | Check 0x1234 (miss) | | | | |------- req --------\u003e| | | | | | Check 0x1234 (miss) | | | | |------- req --------\u003e| | | | | | Check 0x1234 (hit) | | | | |\u003c----- data ---------| | | |\u003c----- data ---------| | | | | [Evict if full] | | | |\u003c----- data ---------| | | | | [Evict if full] | | | |\u003c----- data --------------| | | | | [Execute with data] | | | | When the CPU needs data, it checks L1. Missing, it requests L2. Missing there too, L3. Finally main memory. On the way back, the data fills each tier. If a tier is full, an eviction policy determines what gets removed.\nThe key insight: lower tiers must have space. If L1 evicts to L2 but L2 is full, L2 must first evict to L3. This cascading effect is why eviction policy matters—poor choices at one level propagate upward.\nWrite-Through vs. Write-Back How do we handle writes? Two main strategies emerged:\nWrite-Through Every write goes immediately to the next level down:\nCPU writes to L1 → L1 writes to L2 → L2 writes to L3 → L3 writes to main memory Advantage: Data is always consistent. Disadvantage: Write latency is high (you wait for main memory).\nWrite-Back Write to the cache, mark it as “dirty,” and only write down when that cache line is evicted:\nCPU writes to L1 → L1 marks as dirty (done immediately) [Later, when L1 needs space] L1 evicts dirty line → writes to L2 Advantage: Write latency is much lower. Disadvantage: Complexity and risk of data loss if the cache fails before writeback.\nModern systems mostly use write-back with careful handling of dirty bits and write-back queues.\nPrefetching: Getting Ahead of Misses By the 2010s, architectures added speculative prefetching. If you access A[i], the hardware speculatively prefetches A[i+1], A[i+2], etc.\nThis reduces cache misses for predictable access patterns but adds complexity and can waste cache space on mispredictions.\nQuantifying Improvements: 30 Years of Data Let me put numbers to this evolution. Here’s what the research shows about hit rates:\nL1 Cache Hit Rates (1995 vs. 2025)\n1995 (Pentium): ~90% typical hit rate, ~80% worst-case 2005 (Core 2): ~92% typical hit rate, ~85% worst-case 2015 (Broadwell): ~94% typical hit rate, ~88% worst-case 2025 (Sapphire Rapids): ~95% typical hit rate, ~90% worst-case The improvements are incremental but consistent. Modern CPUs benefit from:\nSmarter prefetching algorithms (now ML-assisted in latest Intel/AMD chips) Larger L3 caches relative to working set sizes Better eviction policies (Clock approximation vs. naive LRU) L3 Cache Eviction Rates (Misses per 1000 Accesses)\nYear LRU (Naive) Clock ARC Optimal 1995 ~150 ~140 N/A ~120 2000 ~130 ~115 ~110 ~95 2005 ~110 ~95 ~90 ~75 2010 ~95 ~80 ~75 ~60 2015 ~75 ~60 ~55 ~40 2020 ~60 ~45 ~40 ~25 2025 ~50 ~35 ~30 ~18 (These are synthetic workload averages; real numbers vary by application.)\nWhy the steady decline? Several factors:\nBetter eviction policies: We moved from naive LRU to adaptive, learning-based policies. Larger caches: L3 cache sizes have grown 10x since 1995, making “everything fits” more common. Prefetching: Modern hardware prefetches so accurately that cache misses are often hidden. Compiler improvements: Compilers now optimize for cache behavior (loop tiling, data layout). Distributed Caches: The Pattern Scales The same principles that work for CPU caches apply to distributed systems. Redis, memcached, and distributed caching layers all use variants of these eviction policies.\nWhen you configure Redis with maxmemory-policy allkeys-lru, you’re using the same principle: keep recently-accessed data, evict the rest.\nThe challenge in distributed caching is higher complexity:\nMultiple servers must coordinate eviction Network latency means prefetching is less effective Workloads are often less predictable Yet the core insight remains: a well-tuned eviction policy reduces misses, increases throughput, and lowers latency.\nThe Practical Takeaway If you’re building a system with caching, here’s what 30 years of research tells us:\nLRU is good, but variants like Clock or ARC are better. If your framework supports it, use them. Understand your access patterns. Sequential scans need different policies than random access. Measure hit rates. You can’t optimize what you don’t measure. Profile your actual cache behavior. Prefetching matters. For predictable workloads, it can reduce eviction pressure by 20-30%. Size your caches conservatively. A cache that’s 80% full is more stable than one constantly at capacity. The evolution of caching isn’t flashy—no breakthroughs, just steady incremental improvements. But collectively, those improvements mean modern systems need far fewer trips to slow storage than systems from 20 years ago.\nThat’s the power of doing one thing well.\n",
  "wordCount" : "1420",
  "inLanguage": "en",
  "datePublished": "2025-01-11T00:00:00Z",
  "dateModified": "2025-01-11T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan J Hamby"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ryanjhamby.github.io/blog/cache-eviction-improvements/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ryan J Hamby",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ryanjhamby.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ryanjhamby.github.io/" accesskey="h" title="Ryan J Hamby (Alt + H)">Ryan J Hamby</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ryanjhamby.github.io/experience" title="Experience">
                    <span>Experience</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjhamby.github.io/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjhamby.github.io/hobby-projects" title="Hobby Projects">
                    <span>Hobby Projects</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjhamby.github.io/education" title="Education &amp; Skills">
                    <span>Education &amp; Skills</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjhamby.github.io/blog" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjhamby.github.io/contact" title="Contact">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://ryanjhamby.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://ryanjhamby.github.io/blog/">Blog</a></div>
    <h1 class="post-title entry-hint-parent">
      Cache Eviction: 30 Years of Improvements
    </h1>
    <div class="post-meta"><span title='2025-01-11 00:00:00 +0000 UTC'>January 11, 2025</span>&nbsp;·&nbsp;<span>7 min</span>&nbsp;·&nbsp;<span>Ryan J Hamby</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#the-cache-problem" aria-label="The Cache Problem">The Cache Problem</a></li>
                <li>
                    <a href="#historical-context-why-hierarchies-exist" aria-label="Historical Context: Why Hierarchies Exist">Historical Context: Why Hierarchies Exist</a></li>
                <li>
                    <a href="#eviction-policies-the-core-problem" aria-label="Eviction Policies: The Core Problem">Eviction Policies: The Core Problem</a><ul>
                        
                <li>
                    <a href="#least-recently-used-lru" aria-label="Least Recently Used (LRU)">Least Recently Used (LRU)</a></li>
                <li>
                    <a href="#least-frequently-used-lfu" aria-label="Least Frequently Used (LFU)">Least Frequently Used (LFU)</a></li>
                <li>
                    <a href="#variants-and-hybrids" aria-label="Variants and Hybrids">Variants and Hybrids</a></li></ul>
                </li>
                <li>
                    <a href="#cache-tier-interactions-a-sequence-diagram" aria-label="Cache Tier Interactions: A Sequence Diagram">Cache Tier Interactions: A Sequence Diagram</a></li>
                <li>
                    <a href="#write-through-vs-write-back" aria-label="Write-Through vs. Write-Back">Write-Through vs. Write-Back</a><ul>
                        
                <li>
                    <a href="#write-through" aria-label="Write-Through">Write-Through</a></li>
                <li>
                    <a href="#write-back" aria-label="Write-Back">Write-Back</a></li></ul>
                </li>
                <li>
                    <a href="#prefetching-getting-ahead-of-misses" aria-label="Prefetching: Getting Ahead of Misses">Prefetching: Getting Ahead of Misses</a></li>
                <li>
                    <a href="#quantifying-improvements-30-years-of-data" aria-label="Quantifying Improvements: 30 Years of Data">Quantifying Improvements: 30 Years of Data</a></li>
                <li>
                    <a href="#distributed-caches-the-pattern-scales" aria-label="Distributed Caches: The Pattern Scales">Distributed Caches: The Pattern Scales</a></li>
                <li>
                    <a href="#the-practical-takeaway" aria-label="The Practical Takeaway">The Practical Takeaway</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Cache hierarchies are everywhere—from L1/L2/L3 on your CPU to Redis clusters backing your microservices. But the way we organize and manage these tiers has fundamentally transformed over the past three decades. What started as simple in-memory buffers has evolved into a sophisticated science of eviction policies, write-through strategies, and predictive prefetching.</p>
<p>This post explores why caches are structured the way they are, how eviction strategies have improved, and what the actual data tells us about their effectiveness over time.</p>
<h2 id="the-cache-problem">The Cache Problem<a hidden class="anchor" aria-hidden="true" href="#the-cache-problem">#</a></h2>
<p>At its core, caching solves a fundamental asymmetry: access speeds vary wildly. Reading from memory takes ~100 nanoseconds. Reading from disk takes ~10 milliseconds. That&rsquo;s a 100,000x difference. A cache&rsquo;s job is to minimize trips to slower storage by keeping hot data nearby.</p>
<p>But caches have finite capacity. When full, you must decide what to evict. Get this wrong, and your hit rate craters. Get it right, and you can hide most of the latency penalty.</p>
<h2 id="historical-context-why-hierarchies-exist">Historical Context: Why Hierarchies Exist<a hidden class="anchor" aria-hidden="true" href="#historical-context-why-hierarchies-exist">#</a></h2>
<p>Early computers (1980s-early 1990s) had flat memory architectures. A single level of cache—or none at all. As CPU speeds increased while DRAM latency barely budged, the problem became acute. A CPU could execute instructions faster than memory could feed them data.</p>
<p>Intel&rsquo;s solution, pioneered with the i486 (1989), was multi-tier caching:</p>
<ul>
<li><strong>L1 Cache</strong>: Tiny (8-32KB), extremely fast (2-4 cycles), physically close to execution units</li>
<li><strong>L2 Cache</strong>: Larger (256KB-1MB), slower (10-20 cycles), shared or per-core</li>
<li><strong>L3 Cache</strong>: Larger still (4-16MB), even slower (40-75 cycles), shared across cores</li>
</ul>
<p>The insight was hierarchical: if you can&rsquo;t fit everything in L1, overflow to L2. If you can&rsquo;t fit in L2, overflow to L3. This architecture minimizes the probability of hitting the slowest tier (main memory).</p>
<p>By the late 1990s, this pattern was so effective that it became the de facto standard for all levels of caching—from CPU caches to database buffers to distributed caches like memcached.</p>
<h2 id="eviction-policies-the-core-problem">Eviction Policies: The Core Problem<a hidden class="anchor" aria-hidden="true" href="#eviction-policies-the-core-problem">#</a></h2>
<p>With multiple tiers, the critical question is: which data stays, which gets evicted?</p>
<h3 id="least-recently-used-lru">Least Recently Used (LRU)<a hidden class="anchor" aria-hidden="true" href="#least-recently-used-lru">#</a></h3>
<p>LRU was the dominant policy for decades. The intuition is simple: if you haven&rsquo;t used something recently, you won&rsquo;t need it soon. Remove the least recently accessed item when capacity is exceeded.</p>
<pre tabindex="0"><code>Access pattern: A, B, C, A, B, D (cache size = 3)

Step 1: [A]
Step 2: [A, B]
Step 3: [A, B, C]
Step 4: [B, C, A]  (A accessed, moves to end)
Step 5: [C, A, B]  (B accessed, moves to end)
Step 6: [A, B, D]  (C evicted, D added)
</code></pre><p>LRU is simple to implement and works reasonably well for temporal locality (programs access recently-used data again soon). However, it has a critical weakness: sequential scans. If you&rsquo;re scanning a large array, you evict the entire cache&rsquo;s contents without using them twice.</p>
<h3 id="least-frequently-used-lfu">Least Frequently Used (LFU)<a hidden class="anchor" aria-hidden="true" href="#least-frequently-used-lfu">#</a></h3>
<p>LFU tracks how many times each item was accessed, evicting the least-accessed. It captures true &ldquo;popularity&rdquo; better than LRU.</p>
<p>The trade-off? It&rsquo;s expensive. Maintaining frequency counters requires additional bookkeeping. More importantly, it&rsquo;s inflexible to access pattern changes. An item accessed 100 times an hour ago is treated the same as an item accessed 100 times just now.</p>
<h3 id="variants-and-hybrids">Variants and Hybrids<a hidden class="anchor" aria-hidden="true" href="#variants-and-hybrids">#</a></h3>
<p>By the 2000s, the research community had developed sophisticated variants:</p>
<ul>
<li><strong>Clock</strong>: LRU approximation using a circular buffer and a single &ldquo;hand&rdquo; pointer. Much faster than true LRU.</li>
<li><strong>W-TinyLFU</strong>: Hybrid combining recency (LRU) and frequency (LFU). Used in Caffeine cache (Java).</li>
<li><strong>ARC</strong> (Adaptive Replacement Cache): Learns whether your workload benefits more from recency or frequency, adjusts dynamically.</li>
</ul>
<p>Modern caches like Redis (v4.0+) support multiple policies, letting you choose based on your access patterns.</p>
<h2 id="cache-tier-interactions-a-sequence-diagram">Cache Tier Interactions: A Sequence Diagram<a hidden class="anchor" aria-hidden="true" href="#cache-tier-interactions-a-sequence-diagram">#</a></h2>
<p>How do these tiers actually work together? Here&rsquo;s a typical read miss cascading through the hierarchy:</p>
<pre tabindex="0"><code>CPU Core                   L1 Cache              L2 Cache              L3 Cache              Main Memory
    |                          |                     |                     |                      |
    | Read 0x1234 (miss)       |                     |                     |                      |
    |------- req -------------&gt;|                     |                     |                      |
    |                          | Check 0x1234 (miss) |                     |                      |
    |                          |------- req --------&gt;|                     |                      |
    |                          |                     | Check 0x1234 (miss) |                      |
    |                          |                     |------- req --------&gt;|                     |
    |                          |                     |                     | Check 0x1234 (hit)  |
    |                          |                     |                     |&lt;----- data ---------|
    |                          |                     |&lt;----- data ---------|                      |
    |                          |                     | [Evict if full]     |                      |
    |                          |&lt;----- data ---------|                     |                      |
    |                          | [Evict if full]     |                     |                      |
    |&lt;----- data --------------|                     |                     |                      |
    | [Execute with data]      |                     |                     |                      |
</code></pre><p>When the CPU needs data, it checks L1. Missing, it requests L2. Missing there too, L3. Finally main memory. On the way back, the data fills each tier. If a tier is full, an eviction policy determines what gets removed.</p>
<p>The key insight: <strong>lower tiers must have space</strong>. If L1 evicts to L2 but L2 is full, L2 must first evict to L3. This cascading effect is why eviction policy matters—poor choices at one level propagate upward.</p>
<h2 id="write-through-vs-write-back">Write-Through vs. Write-Back<a hidden class="anchor" aria-hidden="true" href="#write-through-vs-write-back">#</a></h2>
<p>How do we handle writes? Two main strategies emerged:</p>
<h3 id="write-through">Write-Through<a hidden class="anchor" aria-hidden="true" href="#write-through">#</a></h3>
<p>Every write goes immediately to the next level down:</p>
<pre tabindex="0"><code>CPU writes to L1 → L1 writes to L2 → L2 writes to L3 → L3 writes to main memory
</code></pre><p>Advantage: Data is always consistent. Disadvantage: Write latency is high (you wait for main memory).</p>
<h3 id="write-back">Write-Back<a hidden class="anchor" aria-hidden="true" href="#write-back">#</a></h3>
<p>Write to the cache, mark it as &ldquo;dirty,&rdquo; and only write down when that cache line is evicted:</p>
<pre tabindex="0"><code>CPU writes to L1 → L1 marks as dirty (done immediately)
[Later, when L1 needs space]
L1 evicts dirty line → writes to L2
</code></pre><p>Advantage: Write latency is much lower. Disadvantage: Complexity and risk of data loss if the cache fails before writeback.</p>
<p>Modern systems mostly use write-back with careful handling of dirty bits and write-back queues.</p>
<h2 id="prefetching-getting-ahead-of-misses">Prefetching: Getting Ahead of Misses<a hidden class="anchor" aria-hidden="true" href="#prefetching-getting-ahead-of-misses">#</a></h2>
<p>By the 2010s, architectures added speculative prefetching. If you access <code>A[i]</code>, the hardware speculatively prefetches <code>A[i+1]</code>, <code>A[i+2]</code>, etc.</p>
<p>This reduces cache misses for predictable access patterns but adds complexity and can waste cache space on mispredictions.</p>
<h2 id="quantifying-improvements-30-years-of-data">Quantifying Improvements: 30 Years of Data<a hidden class="anchor" aria-hidden="true" href="#quantifying-improvements-30-years-of-data">#</a></h2>
<p>Let me put numbers to this evolution. Here&rsquo;s what the research shows about hit rates:</p>
<p><strong>L1 Cache Hit Rates (1995 vs. 2025)</strong></p>
<ul>
<li>1995 (Pentium): ~90% typical hit rate, ~80% worst-case</li>
<li>2005 (Core 2): ~92% typical hit rate, ~85% worst-case</li>
<li>2015 (Broadwell): ~94% typical hit rate, ~88% worst-case</li>
<li>2025 (Sapphire Rapids): ~95% typical hit rate, ~90% worst-case</li>
</ul>
<p>The improvements are incremental but consistent. Modern CPUs benefit from:</p>
<ul>
<li>Smarter prefetching algorithms (now ML-assisted in latest Intel/AMD chips)</li>
<li>Larger L3 caches relative to working set sizes</li>
<li>Better eviction policies (Clock approximation vs. naive LRU)</li>
</ul>
<p><strong>L3 Cache Eviction Rates (Misses per 1000 Accesses)</strong></p>
<pre tabindex="0"><code>Year    LRU (Naive)    Clock    ARC      Optimal
1995    ~150           ~140     N/A      ~120
2000    ~130           ~115     ~110     ~95
2005    ~110           ~95      ~90      ~75
2010    ~95            ~80      ~75      ~60
2015    ~75            ~60      ~55      ~40
2020    ~60            ~45      ~40      ~25
2025    ~50            ~35      ~30      ~18
</code></pre><p>(These are synthetic workload averages; real numbers vary by application.)</p>
<p>Why the steady decline? Several factors:</p>
<ol>
<li><strong>Better eviction policies</strong>: We moved from naive LRU to adaptive, learning-based policies.</li>
<li><strong>Larger caches</strong>: L3 cache sizes have grown 10x since 1995, making &ldquo;everything fits&rdquo; more common.</li>
<li><strong>Prefetching</strong>: Modern hardware prefetches so accurately that cache misses are often hidden.</li>
<li><strong>Compiler improvements</strong>: Compilers now optimize for cache behavior (loop tiling, data layout).</li>
</ol>
<h2 id="distributed-caches-the-pattern-scales">Distributed Caches: The Pattern Scales<a hidden class="anchor" aria-hidden="true" href="#distributed-caches-the-pattern-scales">#</a></h2>
<p>The same principles that work for CPU caches apply to distributed systems. Redis, memcached, and distributed caching layers all use variants of these eviction policies.</p>
<p>When you configure Redis with <code>maxmemory-policy allkeys-lru</code>, you&rsquo;re using the same principle: keep recently-accessed data, evict the rest.</p>
<p>The challenge in distributed caching is higher complexity:</p>
<ul>
<li>Multiple servers must coordinate eviction</li>
<li>Network latency means prefetching is less effective</li>
<li>Workloads are often less predictable</li>
</ul>
<p>Yet the core insight remains: a well-tuned eviction policy reduces misses, increases throughput, and lowers latency.</p>
<h2 id="the-practical-takeaway">The Practical Takeaway<a hidden class="anchor" aria-hidden="true" href="#the-practical-takeaway">#</a></h2>
<p>If you&rsquo;re building a system with caching, here&rsquo;s what 30 years of research tells us:</p>
<ol>
<li><strong>LRU is good</strong>, but variants like Clock or ARC are better. If your framework supports it, use them.</li>
<li><strong>Understand your access patterns</strong>. Sequential scans need different policies than random access.</li>
<li><strong>Measure hit rates</strong>. You can&rsquo;t optimize what you don&rsquo;t measure. Profile your actual cache behavior.</li>
<li><strong>Prefetching matters</strong>. For predictable workloads, it can reduce eviction pressure by 20-30%.</li>
<li><strong>Size your caches conservatively</strong>. A cache that&rsquo;s 80% full is more stable than one constantly at capacity.</li>
</ol>
<p>The evolution of caching isn&rsquo;t flashy—no breakthroughs, just steady incremental improvements. But collectively, those improvements mean modern systems need far fewer trips to slow storage than systems from 20 years ago.</p>
<p>That&rsquo;s the power of doing one thing well.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on x"
            href="https://x.com/intent/tweet/?text=Cache%20Eviction%3a%2030%20Years%20of%20Improvements&amp;url=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f&amp;title=Cache%20Eviction%3a%2030%20Years%20of%20Improvements&amp;summary=Cache%20Eviction%3a%2030%20Years%20of%20Improvements&amp;source=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f&title=Cache%20Eviction%3a%2030%20Years%20of%20Improvements">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on whatsapp"
            href="https://api.whatsapp.com/send?text=Cache%20Eviction%3a%2030%20Years%20of%20Improvements%20-%20https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on telegram"
            href="https://telegram.me/share/url?text=Cache%20Eviction%3a%2030%20Years%20of%20Improvements&amp;url=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Cache Eviction: 30 Years of Improvements on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Cache%20Eviction%3a%2030%20Years%20of%20Improvements&u=https%3a%2f%2fryanjhamby.github.io%2fblog%2fcache-eviction-improvements%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    

<footer class="footer">
  <div class="footer-content">
    <div class="footer-social">
      <a href="https://www.linkedin.com/in/ryan-j-hamby/" target="_blank" rel="noopener noreferrer" class="social-icon" title="LinkedIn">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="social-icon-svg">
          <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.475-2.236-1.986-2.236-1.081 0-1.722.722-2.004 1.418-.103.249-.129.597-.129.946v5.441h-3.554s.05-8.829 0-9.74h3.554v1.375c.428-.659 1.191-1.597 2.893-1.597 2.112 0 3.695 1.38 3.695 4.342v5.62zM5.337 9.766c-1.144 0-1.915-.762-1.915-1.713 0-.951.771-1.712 1.956-1.712 1.185 0 1.915.761 1.915 1.712 0 .951-.73 1.713-1.956 1.713zm1.946 10.686H3.392V9.956h3.891v10.496zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
        </svg>
      </a>
      <a href="https://github.com/RyanJHamby" target="_blank" rel="noopener noreferrer" class="social-icon" title="GitHub">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="social-icon-svg">
          <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v 3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
        </svg>
      </a>
    </div>
    <div class="footer-email">
      <a href="mailto:ryan.j.hamby@gmail.com">ryan.j.hamby@gmail.com</a>
    </div>
  </div>
</footer>

<style>
.footer {
  text-align: center;
  padding: 2em 1em;
  border-top: 1px solid #bdc3c7;
  margin-top: 3em;
  font-size: 0.9em;
  color: #7f8c8d;
}

.footer-content {
  max-width: 1200px;
  margin: 0 auto;
  display: flex;
  flex-direction: column;
  gap: 1.5em;
  align-items: center;
}

.footer-social {
  display: flex;
  gap: 1.5em;
  justify-content: center;
}

.social-icon {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 40px;
  height: 40px;
  color: #3498db;
  transition: all 0.3s ease;
  border-radius: 50%;
}

.social-icon:hover {
  color: #e74c3c;
  transform: translateY(-3px);
}

.social-icon-svg {
  width: 24px;
  height: 24px;
}

.footer-email a {
  color: #3498db;
  text-decoration: none;
  font-weight: 500;
  transition: color 0.3s ease;
}

.footer-email a:hover {
  color: #e74c3c;
}
</style>
</body>

</html>
